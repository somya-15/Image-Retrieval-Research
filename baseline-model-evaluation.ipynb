{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline model evaluation","metadata":{}},{"cell_type":"markdown","source":"In order to make sure that model is learning correctly, it is useful to frequently run evaluation loops and keep track whether statistic of interest is being improved. In case of the problem of Landmark Retrieval, the statistic often used is Mean Average Precision (mAP). It is exactly the statistic used for model evaluation in this competiton. It is defined, in [contest evaluation protocol](https://www.kaggle.com/c/landmark-retrieval-2020/overview/evaluation), as:","metadata":{}},{"cell_type":"markdown","source":"![mAP](https://i.imgur.com/ukUp5cy.png)","metadata":{}},{"cell_type":"markdown","source":"In particular, for each query image, embeddings for all index images are compared with the embedding for the query image (using Euclidean distance between embedding vectors), and sorted in a way, such that index images with most similar embedding vectors are placed first. ","metadata":{}},{"cell_type":"markdown","source":"Since the dataset used is very large (>1.5M images), it is not practical to use them all for evaluation - inference on such an enormous dataset takes a significant amount of time. Moreover, it is hard to pick a specific small subset of landmarks to use in evaluation - the dataset is very diverse and it is a challenge to come up with representative set of images.","metadata":{}},{"cell_type":"markdown","source":"In order to address this problem, smaller datasets are used in literature as well. One of those datasets are Revisited Oxford and Paris datasets, presented originally in [paper](https://arxiv.org/abs/1803.11285), as a relabelling of popular [Oxford Buildings Dataset](https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/) and [The Paris Dataset](https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/). The relabeling was a considerate effort, which adressed noisiness present in original datasets. That means that the dataset can be treated with high confidence as a representative one. Moreover, the mAP metrics on these datasets are often reported in majority of recent Image Retrieval literature.","metadata":{}},{"cell_type":"markdown","source":"This notebook worh-> evaluating baseline model provided by Google on Revisited Oxford & Paris datasets. The evaluation loops can be easily re-used for your custom model trained in earlier milestones.","metadata":{}},{"cell_type":"markdown","source":"### 1. Define utility functions","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm  import tqdm\nfrom PIL import Image, ImageFile\nfrom scipy.io import savemat, loadmat\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T13:41:13.579747Z","iopub.execute_input":"2022-05-02T13:41:13.580455Z","iopub.status.idle":"2022-05-02T13:41:19.49721Z","shell.execute_reply.started":"2022-05-02T13:41:13.5804Z","shell.execute_reply":"2022-05-02T13:41:19.495838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\n\nDATASETS = ['roxford5k', 'rparis6k', 'revisitop1m']\n\ndef configdataset(dataset, dir_main):\n\n    dataset = dataset.lower()\n\n    if dataset not in DATASETS:    \n        raise ValueError('Unknown dataset: {}!'.format(dataset))\n\n    if dataset == 'roxford5k' or dataset == 'rparis6k':\n        # loading imlist, qimlist, and gnd, in cfg as a dict\n        gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))\n        with open(gnd_fname, 'rb') as f:\n            cfg = pickle.load(f)\n        cfg['gnd_fname'] = gnd_fname\n        cfg['ext'] = '.jpg'\n        cfg['qext'] = '.jpg'\n\n    elif dataset == 'revisitop1m':\n        # loading imlist from a .txt file\n        cfg = {}\n        cfg['imlist_fname'] = os.path.join(dir_main, dataset, '{}.txt'.format(dataset))\n        cfg['imlist'] = read_imlist(cfg['imlist_fname'])\n        cfg['qimlist'] = []\n        cfg['ext'] = ''\n        cfg['qext'] = ''\n\n    cfg['dir_data'] = os.path.join(dir_main, dataset)\n    cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')\n\n    cfg['n'] = len(cfg['imlist'])\n    cfg['nq'] = len(cfg['qimlist'])\n\n    cfg['im_fname'] = config_imname\n    cfg['qim_fname'] = config_qimname\n\n    cfg['dataset'] = dataset\n\n    return cfg\n\ndef config_imname(cfg, i):\n    return os.path.join(cfg['dir_images'], cfg['imlist'][i] + cfg['ext'])\n\ndef config_qimname(cfg, i):\n    return os.path.join(cfg['dir_images'], cfg['qimlist'][i] + cfg['qext'])\n\ndef read_imlist(imlist_fn):\n    with open(imlist_fn, 'r') as file:\n        imlist = file.read().splitlines()\n    return imlist","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.498836Z","iopub.execute_input":"2022-05-02T13:41:19.499174Z","iopub.status.idle":"2022-05-02T13:41:19.520449Z","shell.execute_reply.started":"2022-05-02T13:41:19.499142Z","shell.execute_reply":"2022-05-02T13:41:19.519213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_ap(ranks, nres):\n    \"\"\"\n    Computes average precision for given ranked indexes.\n    \n    Arguments\n    ---------\n    ranks : zerro-based ranks of positive images\n    nres  : number of positive images\n    \n    Returns\n    -------\n    ap    : average precision\n    \"\"\"\n\n    # number of images ranked by the system\n    nimgranks = len(ranks)\n\n    # accumulate trapezoids in PR-plot\n    ap = 0\n\n    recall_step = 1. / nres\n\n    for j in np.arange(nimgranks):\n        rank = ranks[j]\n\n        if rank == 0:\n            precision_0 = 1.\n        else:\n            precision_0 = float(j) / rank\n\n        precision_1 = float(j + 1) / (rank + 1)\n\n        ap += (precision_0 + precision_1) * recall_step / 2.\n\n    return ap\n\ndef compute_map(ranks, gnd, kappas=[]):\n    \"\"\"\n    Computes the mAP for a given set of returned results.\n         Usage: \n           map = compute_map (ranks, gnd) \n                 computes mean average precsion (map) only\n        \n           map, aps, pr, prs = compute_map (ranks, gnd, kappas) \n                 computes mean average precision (map), average precision (aps) for each query\n                 computes mean precision at kappas (pr), precision at kappas (prs) for each query\n        \n         Notes:\n         1) ranks starts from 0, ranks.shape = db_size X #queries\n         2) The junk results (e.g., the query itself) should be declared in the gnd stuct array\n         3) If there are no positive images for some query, that query is excluded from the evaluation\n    \"\"\"\n\n    map = 0.\n    nq = len(gnd) # number of queries\n    aps = np.zeros(nq)\n    pr = np.zeros(len(kappas))\n    prs = np.zeros((nq, len(kappas)))\n    nempty = 0\n\n    for i in np.arange(nq):\n        qgnd = np.array(gnd[i]['ok'])\n\n        # no positive images, skip from the average\n        if qgnd.shape[0] == 0:\n            aps[i] = float('nan')\n            prs[i, :] = float('nan')\n            nempty += 1\n            continue\n\n        try:\n            qgndj = np.array(gnd[i]['junk'])\n        except:\n            qgndj = np.empty(0)\n\n        # sorted positions of positive and junk images (0 based)\n        pos  = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgnd)]\n        junk = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgndj)]\n\n        k = 0;\n        ij = 0;\n        if len(junk):\n            # decrease positions of positives based on the number of\n            # junk images appearing before them\n            ip = 0\n            while (ip < len(pos)):\n                while (ij < len(junk) and pos[ip] > junk[ij]):\n                    k += 1\n                    ij += 1\n                pos[ip] = pos[ip] - k\n                ip += 1\n\n        # compute ap\n        ap = compute_ap(pos, len(qgnd))\n        map = map + ap\n        aps[i] = ap\n\n        # compute precision @ k\n        pos += 1 # get it to 1-based\n        for j in np.arange(len(kappas)):\n            kq = min(max(pos), kappas[j]); \n            prs[i, j] = (pos <= kq).sum() / kq\n        pr = pr + prs[i, :]\n\n    map = map / (nq - nempty)\n    pr = pr / (nq - nempty)\n\n    return map, aps, pr, prs","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.522334Z","iopub.execute_input":"2022-05-02T13:41:19.522779Z","iopub.status.idle":"2022-05-02T13:41:19.550505Z","shell.execute_reply.started":"2022-05-02T13:41:19.522745Z","shell.execute_reply":"2022-05-02T13:41:19.549453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility script - parsing dataset config .pkl files and mAP computation\n# from roxfordparis_tools import configdataset, compute_map","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.551692Z","iopub.execute_input":"2022-05-02T13:41:19.552406Z","iopub.status.idle":"2022-05-02T13:41:19.569699Z","shell.execute_reply.started":"2022-05-02T13:41:19.552319Z","shell.execute_reply":"2022-05-02T13:41:19.568486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapted from: https://github.com/filipradenovic/revisitop/blob/master/python/example_process_images.py\n\ndef pil_loader(path):\n    # to avoid crashing for truncated (corrupted images)\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    # open path as file to avoid ResourceWarning \n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.573355Z","iopub.execute_input":"2022-05-02T13:41:19.57404Z","iopub.status.idle":"2022-05-02T13:41:19.585309Z","shell.execute_reply.started":"2022-05-02T13:41:19.573988Z","shell.execute_reply":"2022-05-02T13:41:19.58438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapted from: https://github.com/filipradenovic/revisitop/blob/master/python/example_process_images.py\n\ndef extract_features(test_dataset, cfg, model):\n    \"\"\"\n    Generates file with serialized model outputs for each image from test_dataset.\n    \n    Arguments\n    ---------\n    test_dataset   : name of dataset of interest (roxford5k | rparis6k)\n    cfg            : unserialized dataset config, containing annotation metadata\n    model          : loaded Tensorflow baseline model object\n    \"\"\"\n    \n    print('>> Processing query images...', flush=True)\n    Q = []\n    for i in tqdm(np.arange(cfg['nq'])):\n        qim = pil_loader(cfg['qim_fname'](cfg, i)).crop(cfg['gnd'][i]['bbx'])\n        image_data = np.array(qim)\n        image_tensor = tf.convert_to_tensor(image_data)\n        Q.append(model(image_tensor)['global_descriptor'].numpy())\n    Q = np.array(Q, dtype=np.float32)\n    Q = Q.transpose()\n    \n    print('>> Processing index images...', flush=True)\n    X = []\n    for i in tqdm(np.arange(cfg['n'])):\n        im = pil_loader(cfg['im_fname'](cfg, i))\n        image_data = np.array(im)\n        image_tensor = tf.convert_to_tensor(image_data)\n        X.append(model(image_tensor)['global_descriptor'].numpy())\n    X = np.array(X, dtype=np.float32)\n    X = X.transpose()\n\n    feature_dict = {'X': X, 'Q': Q}\n    mat_save_path = \"{}_delg_baseline.mat\".format(test_dataset)\n    print('>> Saving model outputs to: {}'.format(mat_save_path))\n    savemat(mat_save_path, feature_dict)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.587413Z","iopub.execute_input":"2022-05-02T13:41:19.588117Z","iopub.status.idle":"2022-05-02T13:41:19.604233Z","shell.execute_reply.started":"2022-05-02T13:41:19.588059Z","shell.execute_reply":"2022-05-02T13:41:19.603321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapted from: https://github.com/filipradenovic/revisitop/blob/master/python/example_evaluate.py\n\ndef run_evaluation(test_dataset, cfg, features_dir):\n    ks = [1, 5, 10]\n    gnd = cfg['gnd']\n    features = loadmat(os.path.join(features_dir, '{}_delg_baseline.mat'.format(test_dataset)))\n\n    Q = features['Q']\n    X = features['X']\n    sim = np.dot(X.T, Q)\n    ranks = np.argsort(-sim, axis=0)\n\n    # search for easy\n    gnd_t = []\n    for i in range(len(gnd)):\n        g = {}\n        g['ok'] = np.concatenate([gnd[i]['easy']])\n        g['junk'] = np.concatenate([gnd[i]['junk'], gnd[i]['hard']])\n        gnd_t.append(g)\n    mapE, apsE, mprE, prsE = compute_map(ranks, gnd_t, ks)\n\n    # search for easy & hard\n    gnd_t = []\n    for i in range(len(gnd)):\n        g = {}\n        g['ok'] = np.concatenate([gnd[i]['easy'], gnd[i]['hard']])\n        g['junk'] = np.concatenate([gnd[i]['junk']])\n        gnd_t.append(g)\n    mapM, apsM, mprM, prsM = compute_map(ranks, gnd_t, ks)\n\n    # search for hard\n    gnd_t = []\n    for i in range(len(gnd)):\n        g = {}\n        g['ok'] = np.concatenate([gnd[i]['hard']])\n        g['junk'] = np.concatenate([gnd[i]['junk'], gnd[i]['easy']])\n        gnd_t.append(g)\n    mapH, apsH, mprH, prsH = compute_map(ranks, gnd_t, ks)\n\n    print('>> {}: mAP E: {}, M: {}, H: {}'.format(test_dataset, np.around(mapE*100, decimals=2), np.around(mapM*100, decimals=2), np.around(mapH*100, decimals=2)))\n    print('>> {}: mP@k{} E: {}, M: {}, H: {}'.format(test_dataset, np.array(ks), np.around(mprE*100, decimals=2), np.around(mprM*100, decimals=2), np.around(mprH*100, decimals=2)))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.605336Z","iopub.execute_input":"2022-05-02T13:41:19.605735Z","iopub.status.idle":"2022-05-02T13:41:19.6267Z","shell.execute_reply.started":"2022-05-02T13:41:19.605705Z","shell.execute_reply":"2022-05-02T13:41:19.625616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Feature extraction","metadata":{}},{"cell_type":"code","source":"data_root = '/kaggle/input/roxfordparis'\nmodel_root = '/kaggle/input/baseline-landmark-retrieval-model/baseline_landmark_retrieval_model'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-02T13:41:19.628032Z","iopub.execute_input":"2022-05-02T13:41:19.628478Z","iopub.status.idle":"2022-05-02T13:41:19.644246Z","shell.execute_reply.started":"2022-05-02T13:41:19.628445Z","shell.execute_reply":"2022-05-02T13:41:19.643258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.saved_model.load(model_root).signatures['serving_default']","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:19.645553Z","iopub.execute_input":"2022-05-02T13:41:19.64601Z","iopub.status.idle":"2022-05-02T13:41:27.209742Z","shell.execute_reply.started":"2022-05-02T13:41:19.645978Z","shell.execute_reply":"2022-05-02T13:41:27.208569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook uses cached feature extractions from `delg-baseline-roxfordparis-output` Kaggle dataset. Those features can be generated in `/kaggle/working` directory as well, by changing `should_extract_features` flag value to `True`. Be vary that feature extraction takes around ~2h with GPU accelerator on.","metadata":{}},{"cell_type":"code","source":"# Extract features for roxford5k & rparis6k datasets\n\nshould_extract_features = False\n\ndatasets = ['roxford5k', 'rparis6k']\nfor test_dataset in datasets:\n    if should_extract_features:\n        print('Processing dataset: {}'.format(test_dataset), flush=True)\n        cfg = configdataset(test_dataset, data_root)\n        extract_features(test_dataset, cfg, model)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:27.217421Z","iopub.execute_input":"2022-05-02T13:41:27.217947Z","iopub.status.idle":"2022-05-02T13:41:27.228439Z","shell.execute_reply.started":"2022-05-02T13:41:27.217881Z","shell.execute_reply":"2022-05-02T13:41:27.227083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Model evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate on roxford5k & rparis6k datasets\n\ndatasets = ['roxford5k', 'rparis6k']\nfor test_dataset in datasets:\n    print('Evaluating on dataset: {}'.format(test_dataset), flush=True)\n    cfg = configdataset(test_dataset, data_root)\n    run_evaluation(test_dataset, cfg, '/kaggle/input/delg-baseline-roxfordparis-output')\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:27.230203Z","iopub.execute_input":"2022-05-02T13:41:27.230745Z","iopub.status.idle":"2022-05-02T13:41:29.339556Z","shell.execute_reply.started":"2022-05-02T13:41:27.230699Z","shell.execute_reply":"2022-05-02T13:41:29.338347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final mAP metrics are:\n- roxford5k - Easy: 90.92, Medium: 76.23, Hard: 55.54\n- rparis5k - Easy: 94.06, Medium: 87.25, Hard: 74.2\n\nIt's interesting to noticed that these metrics are higher than ones presented in [original DELG paper](https://arxiv.org/pdf/2001.05027.pdf) the baseline model is based on (check rows corresponding to DELG in columns marked as `ROxf` and `RPar`).","metadata":{}},{"cell_type":"markdown","source":"![DELG figure](https://i.imgur.com/IMQZei7.png)","metadata":{}},{"cell_type":"markdown","source":"### BONUS: Testing model with concrete input","metadata":{}},{"cell_type":"markdown","source":"The following code cells perform image retrieval of 5 most similar images using baseline model.","metadata":{}},{"cell_type":"code","source":"def get_random_image(test_dataset, cfg):\n    query_image_id = random.choice(range(cfg['nq']))\n    im = pil_loader(cfg['im_fname'](cfg, query_image_id))\n    return query_image_id, im","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:29.34098Z","iopub.execute_input":"2022-05-02T13:41:29.34139Z","iopub.status.idle":"2022-05-02T13:41:29.348418Z","shell.execute_reply.started":"2022-05-02T13:41:29.341353Z","shell.execute_reply":"2022-05-02T13:41:29.347012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_inference(pil_image, model):\n    image_data = np.array(pil_image)\n    image_tensor = tf.convert_to_tensor(image_data)\n    return model(image_tensor)['global_descriptor'].numpy()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:29.350043Z","iopub.execute_input":"2022-05-02T13:41:29.350387Z","iopub.status.idle":"2022-05-02T13:41:29.361726Z","shell.execute_reply.started":"2022-05-02T13:41:29.350356Z","shell.execute_reply":"2022-05-02T13:41:29.360484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = 'roxford5k' # (roxford5k | rparis6k)\ncfg = configdataset(test_dataset, data_root)\n\nim_id, im = get_random_image(test_dataset, cfg)\nim_feats = run_inference(im, model)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:29.363333Z","iopub.execute_input":"2022-05-02T13:41:29.363777Z","iopub.status.idle":"2022-05-02T13:41:37.909353Z","shell.execute_reply.started":"2022-05-02T13:41:29.363731Z","shell.execute_reply":"2022-05-02T13:41:37.908369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_dir = '/kaggle/input/delg-baseline-roxfordparis-output'\n\nfeatures = loadmat(os.path.join(features_dir, '{}_delg_baseline.mat'.format(test_dataset)))\nX = features['X']\nsim = np.dot(X.T, im_feats)\nranks = np.argsort(-sim, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:37.910447Z","iopub.execute_input":"2022-05-02T13:41:37.910761Z","iopub.status.idle":"2022-05-02T13:41:37.94168Z","shell.execute_reply.started":"2022-05-02T13:41:37.910731Z","shell.execute_reply":"2022-05-02T13:41:37.940823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.array(im))\nplt.title('Query Image')\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:37.943067Z","iopub.execute_input":"2022-05-02T13:41:37.943388Z","iopub.status.idle":"2022-05-02T13:41:38.154402Z","shell.execute_reply.started":"2022-05-02T13:41:37.943355Z","shell.execute_reply":"2022-05-02T13:41:38.153577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(16, 12))\ncolumns = 5\nrows = 1\nfor i in range(1, columns*rows +1):\n    img = np.array(pil_loader(cfg['im_fname'](cfg, ranks[i-1])))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis('off')\nprint('Five most similar images from query dataset:')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:41:38.155767Z","iopub.execute_input":"2022-05-02T13:41:38.156294Z","iopub.status.idle":"2022-05-02T13:41:39.284927Z","shell.execute_reply.started":"2022-05-02T13:41:38.156251Z","shell.execute_reply":"2022-05-02T13:41:39.283686Z"},"trusted":true},"execution_count":null,"outputs":[]}]}